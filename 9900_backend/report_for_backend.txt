Methodology

A. Content Database
  Our content Database consists of chunks of data from course lecture notes. A chunk, which can be returned
to the user as a query's answer, is a piece of content under a specific heading topic in lecture notes.
In addition, these headings are also stored in our Database to help our system find answers with respective
to some direct questions.

B. Keyword Extraction
  Keyword Extraxtion model works in two steps. One is to construct a corpus using data from content Database.
Another is to extract some words that have more relative importance from chunks. This is also used for queries,
which improves efficience of following compute. We use TF.IDF to evaluate the importance of a word. Term frequency 
provides us with the importance of a word within a chunk. Meanwhile, long term frequency is applied for properly 
reducing difference among words. Moreover, our approach use inverses document frequency to make sure that rare terms 
are more informative than frequent terms within all chunks.

  In our approach, data from content database firstly is preprocessed using tokenization, normalization, 
lemmatization and stemming. Then those preprocessed chunks and headings are treated as documents and constitute 
the corpus for our system. Terms with the tf.idf more than 0.15 will be kept intended for answer. Those documents 
and their kept terms are intended for answer retrieval in the following work.

C. Information Retrieval

  Queries related to course content, identified by Dialogflow, will be passed to our information retrieval model.
Others will be answered as general questions and our agent will return some presupposed or default information
to users. Those content-related queries will be preprocessed firstly and then the system also extracted keywords
using our keyword extraction model. An extra restriction is that keywords must be in our term-dictionary of our 
corpus.

  Each document, including headings and content, is now represented by a real-valued vector of tf.idf weights
during the process of retrieval. The query, which is waiting for searching an answer, is treated as a vector 
now, as well. Then we evaluate the cosine distance between each document and the query. Cosine distance is a 
effective method to commute the similarity of two vectors. It more considers the distribution of terms rather 
than length of vectors. In practice, our system apply L2 normalization to weight of terms. The whole compute is 
mathematically given by equation below:

Score(vector_q, vector_c) = sum(q_i * c_i * theta_i for i in vector) / sqrt(sum(square(q_i) for i in vector)) * sqrt(sum(square(c_i) for i in vector))

Finally, two or three data chunks with the highest cosine score will be returned to the user, as the answer of 
this query.


