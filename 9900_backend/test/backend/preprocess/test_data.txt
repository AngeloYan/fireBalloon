1. massively parallel distributed process or made up of simple processing units 2. knowledge acquired from environment through a learning process 3. knowledge stored in the form of synaptic weights
1. biologically inspired 2. good learning properties 3. continuous, nonlinear 4. well adapted to certain tasks 5. fault tolerant 6. graceful degradation
1. 380BC Plato (Rationalism - innateness) 2. 330BC Aristotle (Empricism - experience) 3. 1641 Descartes (mind-body Dualism) 4. 1781 Kant (Critique of Pure Reason) 5. 1899 Sigmund Freud (Psychology) 6. 1953 B.F. Skinner (Behaviourism)
1. 1642 Blaise Pascal (mechanical adding machine) 2. 1694 Gottfried Leibniz (mechanical calculator) 3. 1769 Wolfgang von Kempelen (Mechanical Turk) 4. 1837 Charles Babbage & Ada Lovelace (Difference Engine) 5. 1848 George Boole (the Calculus of Logic) 6. 1879 Gottlob Frege (Predicate Logic) 7. 1950 Turing Test 8. 1956 Dartmouth conference
1. 1943 McCulloch & Pitts (neuron models) 2. 1948 Norbert Wiener (Cybernetics) 3. 1948 Alan Turing (B-Type Networks) 4. 1955 Oliver Selfridge (Pattern Recognition) 5. 1962 Hubel and Wiesel (visual cortex) 6. 1962 Frank Rosenblatt (Perceptron)
1. 1956 Newell & Simon (Logic Theorist) 2. 1959 John McCarthy (Lisp) 3. 1959 Arther Samuel (Checkers) 4. 1965 Joseph Weizenbaum (ELIZA) 5. 1967 Edward Feigenbaum (Dendral)
1. 1969 Minsky & Papert published Perceptrons, emphasizing the limitations of neural models, and lobbied agencies to cease funding neural network research. 2. from 1969 to 1985 there was very little work in neural networks or machine learning. 3. a few exceptions, e.g. Stephen Grossberg, Teuvo Kohonen (SOM), Paul Werbos.
1. 1970s and early 1980s, AI research focused on symbolic processing, Expert Systems 2. Some commercial success, but ran into difficulties:(1). combinatorial explosion in search spaces (2). difficulty of formalising everyday knowledge as well as expert knowledge
1. 1986 Rumelhart, Hinton & Williams (multi-layer, backprop) 2. 1989 Dean Pomerleau (ALVINN) 3. late 1980’s renewed enthusiasm, hype 4. 1990s more principled approaches 5. 2000’s SVM, Bayesian models became more popular 6. 2010’s deep learning networks, GPU’s 7. 2020’s spiking networks(?)
1. Image processing (1). classification (2). segmentation 2.Language processing (1). translation (2). semantic disambiguation (3). sentiment analysis 3. Combining images and tex (1). automatic captioning 4. Game playing (1). AlphaGo (2). Deep Q-Learning
Two perspectives on the history of Deep Learning Viewpoint 1: Focusing on recent work (after 2012) https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf Viewpoint 2: Focusing on earlier work (before 2012) http://people.idsia.ch/~juergen/deep-learning-overview.html
1. Central Nervous System (1). Brain (2).Spinal cord 2. Peripheral Nervous System (1). Somatic nervous system (2). Autonomic nervous system (3). Enteric nervous system
1. “cortex” from Latin word for “bark” (of tree) 2. cortex is a sheet of tissue making up outer layers of brain , 2-6cmthick 3. right and left sides connected by corpus callosum 4. functions: thought, voluntary movement, language, reasoning, perception
1. general term for area of brain between the thalamus and spinal cord 2. includes medulla, pons, tectum, reticular formation and tegmentum 3. functions: breathing, heart rate, blood pressure, and others
1. from Latin word for “little brain” 2. functions: movement, balance, posture
functions: vision, audition, eye movement, body movement
1. receives sensory information and relays it to the cerebral cortex 2. also relays information from the cerebral cortex to other areas of the brain, and the spinal cord 3. functions: sensory integration, motor integration
1. composed of several different areas at the base of the brain 2. the size of a pea (about 1/300 of the total brain weight) 3. functions: body temperature, emotions, hunger, thirst, circadian rhythms
1. group of structures including amygdala, hippocampus, mammillary bodies and cingulate gyrus 2. important for controlling the emotional response to a given situation 3. hippocampus also important for memory 4. functions: emotional behaviour
1. The body is made up of billions of cells. Cells of the nervous system, called neurons, are specialized to carry “messages” through an electrochemical process. 2. The human brain has about 100 billion neurons, and a similar number of support cells called “glia”. 3. Neurons are similar to other cells in the body in some ways, such as: (1). neurons are surrounded by a cell membrane (2). neurons have a nucleus that contains genes (DNA) (3). neurons carry out basic cellular processes like protein synthesis and energy production
1. Neurons have specialized extensions called dendrites and axons Dendrites bring information to the cell body, while axons take information away from the cell body. 2. The axon of one neuron can connect to the dendrite of another neuron through an electrochemical junction called a synapse. 3. Most neurons have only one axon, but the number of dendrites can vary widely: (1). Unipolar and Bipolar neurons have only one dendrite (2). Purkinje neurons can have up to 100,000 dendrites
1. Dendrites are typically less than a millimetre in length 2. Axons can vary in length from less than a millimetre to more than a metre (motor neurons) 3. Long axons are sometimes surrounded by a myelinated sheath, which prevents the electrical signal from dispersing, and allows it to travel faster (up to 100 m/s).
1. electrical pulse reaches the endbulb and causes the release of neurotransmitter molecules from little packets (vesicles) through the synaptic membrane 2. transmitter then diffuses through the synaptic cleft to the other side 3. when the neurotransmitter reaches the post-synaptic membrane, it causes a change in polarisation of the membrane 4. the change in potential can be excitatiory (moving the potential towards the threshold) or inhibitory (moving it away from the threshold)
1. human brain has 100 billion neurons with an average of 10,000 synapses each 2. latency is about 3-6 milliseconds 3. therefore, at most a few hundred “steps” in any mental computation, but massively parallel
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNN’s can now be simulated with massive parallelism, using GPU’s
1. Suppose we want to classify an image as a bird, sunset, dog, cat, etc. 2. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. 2. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image.
1. can “unroll” a recurrent architecture into an equivalent feedforward architecture, with shared weights 2. useful for processing language or other temporal sequences
1. output is trained to reproduce the input as closely as possible 2. activations normally pass through a bottleneck, so the network is forced to compress the data in some way
1. biological neurons spike in different patterns (quiescent, persistent, sporadic) 2. spike timing might carry important information 3. most NN models ignore timing information, but some work has been done on spiking network models 4. in the future, special hardware might lead to a revolution for spiking networks, similar to what GPU’s provided for CNN’s
1. Neurons – Biological and Artificial 2. Perceptron Learning 3. Linear Separability 4. Multi-Layer Networks
1. The brain is made up of neurons (nerve cells) which have (1). a cell body (soma) (2). dendrites (inputs) (3). an axon (outputs) (4). synapses (connections between cells) 2. Synapses can be exitatory or inhibitory and may change over time. When the inputs reach some threshhold an action potential (electrical pulse) is sent along the axon to the outputs.
1.(Artificial) Neural Networks are made up of nodes which have (1). inputs edges, each with some weight (2). outputs edges (with weights) (3). an activation level (a function of the inputs) 2. Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: Some nodes are inputs (sensing), some are outputs (action)
w0=-th, s = w1x1 + w2x2−th = w1x1 + w2x2 + w0, x1, x2 are inputs, w1, w2 are synaptic weights, th is a threshold, w0 is a bias weight, g is transfer function
linearly separable functions
In 1969, Minsky and Papert published a book highlighting the limitations of Perceptrons, and lobbied various funding agencies to redirect funding away from neural network research, preferring instead logic-based methods such as expert systems. It was known as far back as the 1960’s that any given logical function could be implemented in a 2-layer neural network with step function activations. But, the the question of how to learn the weights of a multi-layer neural network based on training examples remained an open problem. The solution, which we describe in the next section, was found in 1976 by Paul Werbos, but did not become widely known until it was rediscovered in 1986 by Rumelhart, Hinton and Williams.
1. Supervised Learning 2. Ockham’s Razor (5.2) 3. Multi-Layer Networks 4.  Gradient Descent (4.3, 6.5.2)
1. Supervised Learning: agent is presented with examples of inputs and their target outputs 2. Reinforcement Learning: agent is not presented with target outputs, but is given a reward signal, which it aims to maximize 3. Unsupervised Learning: agent is only presented with the inputs themselves, and aims to find structure in these inputs
1. we have a training set and a test set, each consisting of a set of items; for each item, a number of input attributes and a target value are specified. 2. the aim is to predict the target value, based on the input attributes. 3. agent is presented with the input and target output for each item in the training set; it must then predict the output for each item in the test set 4. various learning paradigms are available: (1). Neural Network (2). Decision Tree (3). Support Vector Machine, etc.
1. framework (decision tree, neural network, SVM, etc.) 2.  representation (of inputs and outputs) 3. pre-processing / post-processing 4. training method (perceptron learning, backpropagation, etc.) 5. generalization (avoid over-fitting) 6. evaluation (separate training and testing sets)
“The most likely hypothesis is the simplest one consistent with the data.” Since there can be noise in the measurements, in practice need to make a tradeoff between simplicity of the hypothesis and how well it fits the data.
x1 XOR x2 can be written as: (x1 AND x2) NOR (x1 NOR x2) Recall that AND, OR and NOR can be implemented by perceptrons.
Normally, the numbers of input and output units are fixed, but we can choose the number of hidden units.
for this toy problem, there is only a training set; there is no validation or test set, so we don’t worry about overfitting the XOR data cannot be learned with a perceptron, but can be achieved using a 2-layer network with two hidden units
We define an error function E to be (half) the sum over all input patterns of the square of the difference between actual output and desired output If we think of E as height, it defines an error landscape on the weight space. The aim is to find a set of weights for which E is very low.
because of the step function, the landscape will not be smooth but will instead consist almost entirely of flat local regions and “shoulders”, with occasional discontinuous jumps.
Recall that the error function E is (half) the sum over all input patterns of the square of the difference between actual output and desired output The aim is to find a set of weights for which E is very low. If the functions involved are smooth, we can use multi-variable calculus to adjust the weights in such a way as to take us in the steepest downhill direction. Parameter η is called the learning rate. 
This principle can be used to compute the partial derivatives in an efficient and localized manner. Note that the transfer function must be differentiable (usually sigmoid, or tanh).
1. Medical Dignosis 2. Autonomous Driving 3. Game Playing 4. Credit Card Fraud Detection 5. Handwriting Recognition 6. Financial Prediction
1. re-scale inputs and outputs to be in the range 0 to 1 or −1 to 1 2. replace missing values with mean value for that attribute 3. initialize weights to very small random values 4. on-line or batch learning 5. three different ways to prevent overfitting: (1). limit the number of hidden nodes or connections (2). limit the training time, using a validation set (3). weight decay 6. adjust learning rate (and momentum) to suit the particular task
1. Autonomous Land Vehicle In a Neural Network 2.  later version included a sonar range finder (1). 8×32 range finder input retina (2). 29 hidden units (3). 45 output units 3. Supervised Learning, from human actions (Behavioral Cloning) additional “transformed” training items to cover emergency situations 4. drove autonomously from coast to coast
1. Neural networks are biologically inspired 2. Multi-layer neural networks can learn nonlinearly separable functions 3. Backpropagation is effective and widely used
1. Probability (3.1-3.6, 3.9.3, 3.10) 2. Cross Entropy (5.5) 3. Bayes’ Rule (3.11) 4. Weight Decay (5.2.2) 5. Momentum (8.3)
Begin with a set Ω – the sample space (e.g. 6 possible rolls of a die) ω ∈ Ω is a sample point/possible world/atomic event A probability space or probability model is a sample space with an assignment P(ω) for every ω ∈ Ω s.t.
A random variable (r.v.) is a function from sample points to some range (e.g. the Reals or Booleans) For example, Odd(3) = true.
1. cross entropy (1). problem: least squares error function unsuitable for classification, where target = 0 or 1 (2). mathematical theory: maximum likelihood (3). solution: replace with cross entropy error function 2. Weight Decay (1). problem: weights “blow up”, and inhibit further learning (2). mathematical theory: Bayes’ rule (3). solution: add weight decay term to error function 3. Momentum (1). problem: weights oscillate in a “rain gutter” (2). solution: weighted average of gradient over time
For classification tasks, target t is either 0 or 1, so better to use E = −t log(z −(1−t)log(1−z) This can be justified mathematically, and works well in practice – especially when negative examples vastly outweigh positive ones. It also makes the backprop computations simpler.
H is a class of hypotheses P(D|h) = probability of data D being generated under hypothesis h ∈ H. logP(D|h) is called the likelihood. ML Principle: Choose h ∈ H which maximizes the likelihood, i.e. maximizes P(D|h) [or, maximizes logP(D|h)]
The formula for conditional probability can be manipulated to find a relationship when the two variables are swapped: P(a∧b) = P(a|b)P(b) = P(b a)P(a) This is often useful for assessing the probability of an underlying cause after an effect has been observed:
Question: Suppose we have a test for a type of cancer which occurs in 1% of patients. The test has a sensitivity of 98% and a specificity of 97%. If a patient tests positive, what is the probability that they have the cancer Answer: There are two random variables: Cancer (true or false) and Test (positive or negative). The probability is called a prior, because it represents our estimate of the probability before we have done the test (or made some other observation). The sensitivity and specificity are interpreted as follows: P(positive|cancer) = 0.98, and P(negative|¬cancer) = 0.97
Question: You work for a lighting company which manufactures 60% of its light bulbs in Factory A and 40% in Factory B. One percent of the light bulbs from Factory A are defective, while two percent of those from Factory B are defective. If a random light bulb turns out to be defective, what is the probability that it was manufactured in Factory A? Answer: There are two random variables: Factory (A or B) and Defect (Yes or No). In this case, the prior is: P(A) = 0.6, P(B) = 0.4 The conditional probabilities are: P(defect|A) = 0.01, and P(defect|B) = 0.02
H is a class of hypotheses P(D|h) = probability of data D being generated under hypothesis h ∈ H. P(h|D) = probability that h is correct, given that data D were observed. Bayes’ Theorem: P(h) is called the prior.
Assume that small weights are more likely to occur than large weights, i.e. where Z is a normalizing constant. Then the cost function becomes: This can prevent the weights from “saturating” to very high values. Problem: need to determine λ from experience, or empirically.
If landscape is shaped like a “rain gutter”, weights will tend to oscillate without much improvement. Solution: add a momentum factor
Compute matrix of second derivatives (called the Hessian). Approximate the landscape with a quadratic function (paraboloid). Jump to the minimum of this quadratic function.
Use methods from information geometry to find a “natural” re-scaling of the partial derivatives.